{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define functions for random forest model\n",
    "\n",
    "Random Forest models are ensemble machine learning algorithms widely used for classification and regression predictive modelling problems.  They are an ensemble of [decision tree algorithms](https://en.wikipedia.org/wiki/Decision_tree_learning) that can fit on slightly different examples of a single dataset in an attempt to produce a better overall result.  To validate these means, K-fold cross validation is often used.  This process involves taking random samples of the data and retraining the model so as not to overfit to one training sample.  Unfortunately, this method is not necessarily fit for modelling time series data as it can lead to overly optimistic models by sample future data and predicting on the past.\n",
    "\n",
    "Jason Brownlee [documents a workflow](https://machinelearningmastery.com/random-forest-for-time-series-forecasting/) for Random Forest Time Series Forecasting which is used in this notebook to explore the feasibility of it for predicting ambient population. This will be discussed as the data is processed and the models run later.\n",
    "\n",
    "A note on cross validation - Whilst the use of it for time series has been found to be feasible by Bergmeir et al (2018) by only using lagged versions of the outcome variable, it would be unsuitable for this project which seeks to use other explanatory variables alongside these.\n",
    "\n",
    "Please note - <b>The Walk Forward Validation used later can take a long time and hog pc resources depending on the size of the dataset, number of trees set and time lags to use.  Please be selective in which sections of the notebook you run.  To avoid any problems with running it in batches, only run up to the cells that undertake validation.  These are clearly marked with a note later.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sklearn\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "from plotly.subplots import make_subplots\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import lag_plot\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import os\n",
    "\n",
    "from source import *\n",
    "\n",
    "# forecast monthly births with random forest\n",
    "\n",
    "if not os.path.exists(\"../images\"):\n",
    "    os.mkdir(\"../images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data and define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import merged footfall data\n",
    "footfalldf_imported = pd.read_csv(\"../data/LCC_footfall_2021.gz\",\n",
    "                                  parse_dates=['Date','DateTime'],\n",
    "                                  dtype={\"BRCYear\": int,\"BRCWeekNum\":int})\n",
    "\n",
    "\n",
    "new_weather = pd.read_csv(\"../data/weatherdata.csv\",parse_dates=['timestamp'],index_col='timestamp')\n",
    "previous_weather = pd.read_csv(\"../data/overall_weather.csv\",parse_dates=['date'],index_col='date',dayfirst=True)\n",
    "bankhols = pd.read_csv('../data/ukbankholidays.csv',parse_dates=['ukbankhols'])\n",
    "schoolterms = pd.read_csv('../data/schoolterms.csv',parse_dates=['date'],dayfirst=True,index_col='date',usecols=['schoolStatus','date'])\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "def model1_validation(laglist,footfall,tree):\n",
    "    mae_list = []\n",
    "    validation_df = pd.DataFrame()\n",
    "    for lag in laglist:\n",
    "\n",
    "        n_in=lag\n",
    "        # transform the time series data into supervised learning and add additional explanatory variables\n",
    "        processed_data = (footfall\n",
    "                          .pipe(start_pipeline)\n",
    "                          .pipe(series_to_supervised,n_in=n_in)\n",
    "                          .pipe(create_lockdown_predictors)\n",
    "                          .pipe(create_date_predictors)\n",
    "                          .pipe(create_holiday_predictors,bankhols,schoolterms)\n",
    "                          .pipe(create_weather_predictors, new_weather, previous_weather)\n",
    "                          .pipe(drop_na)\n",
    "                          .pipe(arrange_cols,n_in=n_in))\n",
    "\n",
    "        cols_to_scale = ['mean_temp', 'wind_speed', 'rain', 'hosp_indoor', 'hosp_outdoor', 'hotels', 'ent_indoor', 'ent_outdoor',\n",
    "                         'weddings', 'self_acc', 'sport_lei_indoor', 'sport_lei_outdoor', 'non_ess_retail', 'prim_sch',\n",
    "                         'sec_sch', 'uni_campus', 'outdoor_grp_public', 'outdoor_grp_private', 'indoor_grp', 'eat_out']\n",
    "\n",
    "        #variables to count how many days fall after 23rd March 2020\n",
    "        lockdown_days = len(processed_data.loc[processed_data.index >= \"2020-03-23\"])\n",
    "        #variable to count how many days fall after 1st January 2021\n",
    "        model1_test_days = len(processed_data.loc[processed_data.index >= \"2021-01-01\"])\n",
    "\n",
    "        #evaluate model using walk forward validation.  The default days are set at 12 for now.  To set a custom number, please change n_test.  To use all days after 1st January 2021 please set n_test to model1_test_days.  NOTE THIS WILL TAKE A LONG TIME TO VALIDATE.\n",
    "        mae, y, yhat = walk_forward_validation(processed_data, 30, cols_to_scale, n_in,tree)\n",
    "        mae_list.append(mae)\n",
    "        validation_df[f'timelag_{lag}'] = yhat\n",
    "\n",
    "    return mae_list, validation_df, y\n",
    "\n",
    "def model2_validation(laglist,footfall,tree):\n",
    "    mae_list = []\n",
    "    validation_df = pd.DataFrame()\n",
    "    for lag in laglist:\n",
    "\n",
    "        n_in=lag\n",
    "        # transform the time series data into supervised learning and add additional explanatory variables\n",
    "        processed_data = (footfall\n",
    "                      .pipe(start_pipeline)\n",
    "                      .pipe(series_to_supervised,n_in=n_in)\n",
    "                      .pipe(create_date_predictors)\n",
    "                        .pipe(create_holiday_predictors,bankhols,schoolterms)\n",
    "                        .pipe(create_weather_predictors, new_weather, previous_weather)\n",
    "                      .pipe(drop_na)\n",
    "                      .pipe(arrange_cols,n_in=n_in))\n",
    "\n",
    "        #create list of columns that need scaling\n",
    "        cols_to_scale = ['mean_temp', 'wind_speed', 'rain']\n",
    "\n",
    "        #split dataset based on when first measures to reduce social contact were put in place.\n",
    "        prelockdown_data = processed_data.loc[processed_data.index < \"2020-03-16\"]\n",
    "\n",
    "        #evaluate model using walk forward validation.  The default days are set at 12 for now.  To set a custom number, please change n_test.  To use all days after 1st January 2021 please set n_test to model1_test_days.  NOTE THIS WILL TAKE A LONG TIME TO VALIDATE.\n",
    "        mae, y, yhat = walk_forward_validation(prelockdown_data, 30, cols_to_scale, n_in,tree)\n",
    "        mae_list.append(mae)\n",
    "        validation_df[f'timelag_{lag}'] = yhat\n",
    "\n",
    "    return mae_list, validation_df, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "The next step in the pipeline is to check for duplicates and remove them.  Initial data exploration revealed errors in some of the csv files where individual records had been duplicated.  In some instances, the same records existed in several different files, for example dates in early July appeared towards the end of the June csv.\n",
    "\n",
    "The cameras don't all come online at the same time, with the last starting on 27th August 2008.  To ensure meaningful comparability, any records before this date have been removed.\n",
    "\n",
    "Finally, one of the cameras appeared to have moved locations on 31st May 2015 from Commercial Street at Lush to Commercial Street at Sharps.  These are combined and renamed to Commercial Street Combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footfall hasn't changed when combining cameras\n",
      "There are 0 duplicates left\n"
     ]
    }
   ],
   "source": [
    "#Pipeline that imports csv files, creates a dataframe and applies cleaning functions\n",
    "footfalldf = (footfalldf_imported\n",
    "              .pipe(start_pipeline)\n",
    "              .pipe(set_start_date, '2008-08-27')\n",
    "              .pipe(combine_cameras)\n",
    "              .pipe(check_remove_dup)\n",
    "              .pipe(remove_new_cameras)\n",
    "              .pipe(create_BRC_MonthNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Resample into daily footfall.\n",
    "daily_footfall = footfalldf.groupby( [pd.Grouper(key='DateTime',freq='D')])['Count'].sum().to_frame()\n",
    "#dayfinal = pd.concat([day,frame],verify_integrity=True)\n",
    "daily_footfall = daily_footfall.drop(daily_footfall[daily_footfall['Count'] == 0].index)\n",
    "#Set frequency to daily, creating additional rows for missing values and impute using the 'time' based interpolation\n",
    "daily_footfall = daily_footfall.asfreq('D').dropna()#.replace(0,np.nan).interpolate(method='time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Models for Footfall\n",
    "\n",
    "The following code blocks generate two Random Forest Models.\n",
    "\n",
    "The first is trained on all available data up to the most recent (currently April 2021).  It includes lockdown specific conditions and other explanatory variables such as lagged outcomes, weather and time series specific inputs such as day of the week or month of the year. This could quantify how important lockdown variables were in explaining the changes in footfall that occur during periods of lockdown.\n",
    "\n",
    "The second model is trained just on data up to the beginning of lockdown and ignore the conditions, given that it would be predicting 'business as usual'.  Essentially they're pretending the pandemic never happened so only need to consider the elements that were found to be important in [previous modelling of footfall data](https://github.com/nickmalleson/lcc-footfall/blob/master/LCC_Footfall.ipynb) by [Molly Asher](https://eps.leeds.ac.uk/civil-engineering/pgr/7524/molly-asher).\n",
    "\n",
    "This approach explores several avenues.  First, it covers quantifying the changes and explains general footfall and then cover making predictions.  It would be useful to have the model with lockdown variables available for future prediction if they turn out to have good explanatory power compared with the usual predictors.\n",
    "\n",
    "#### Model #1 - Explanation of lockdown variable strength when considering changes in footfall during pandemic period.\n",
    "\n",
    "To create the models, data is processed as follows:\n",
    "\n",
    "- Data is transformed into a supervised learning problem, with various time lags created using pandas.shift().  These lags become X input variables, with the original counts becoming a y output.  Each row in the dataset contains however many time shifts you've calculated as X inputs, plus any additional variables you'd like to provide explanatory power\n",
    "- Creation of lockdown predictors to represent the different parts of society that were closed during lockdowns since March 2020. They cover things such as non-essential retail, hospitality, entertainment, sport/leisure, weddings, education and maximum group sizes.  These are a mix of label encoded and dummy variables.  Some of these might be subject to change depending on model performance\n",
    "- Creation of Date predictors, time series specific input variables such as day of week, month of year or even whether the day is a weekend or not\n",
    "- Creation of Holiday predictors that represent whether the day falls on a bank or school holiday\n",
    "- Creation of Weather predictors that consist of mean daily temperature, total daily rainfall and mean daily wind speed\n",
    "- Removal of missing values\n",
    "- Arrange columns so that the observed y variable is the last in the dataframe.  This is required for walk-forward validation later on\n",
    "\n",
    "This is undertaken for each model, with some changes that will be discussed in the relevant markdown section.\n",
    "\n",
    "#### Walk Forward Validation\n",
    "\n",
    "The walk-forward validation method iterates through a test set explicitly specified as being towards the end of the data sequence using the explanatory variables generated to predict values, comparing against expected and providing an MAE.  Once all the parameters have been set and model finalised, then variable importance can be extracted and predictions on out of sample data can be made.\n",
    "\n",
    "<b>Please note that this method is relatively computationally expensive</b> as it loops through each row and retrains the model to include additional footfall values.  Only run if you want to see the outputs.  It is not required to see the results of the actual model.\n",
    "\n",
    "The code can be adjusted to account for more/less lags and trees.  These are just for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation has started on 100 trees with 1 time lag(s).  Please be patient, it may take a while and a message will be displayed when finished.\n",
      "Validation has started on 100 trees with 3 time lag(s).  Please be patient, it may take a while and a message will be displayed when finished.\n",
      "Validation has started on 100 trees with 7 time lag(s).  Please be patient, it may take a while and a message will be displayed when finished.\n"
     ]
    }
   ],
   "source": [
    "#Run model 1 validation function using a range of trees\n",
    "lags = [1,3,7]\n",
    "trees = [100,200,500,1000]\n",
    "figs = []\n",
    "mae_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for tree in trees:\n",
    "    mae_list,validation_df,y = model1_validation(lags,daily_footfall,tree)\n",
    "    validation_df['y'] = y\n",
    "    fig = px.line(validation_df,title=f'Model 1 Validation - {tree} trees')\n",
    "    figs.append(fig)\n",
    "    fig.write_image(f\"../images/Model1Validation{tree}.svg\")\n",
    "    mae_df[f'{tree}_trees'] = mae_list\n",
    "    mae_df.to_csv(f\"../data/model1mae.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plots the Mean Absolute Error for each number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: [1, 2, 3, 4]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-5180c465cb50>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmae_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../data/model1mae.csv\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0musecols\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mmae_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmae_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrename\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;34m'1 Lag'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'3 Lag'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'7 Lag'\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmae_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfacet_col\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'variable'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfor_each_annotation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"=\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\amb_pop\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    609\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 610\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    611\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    612\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\amb_pop\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    461\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 462\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    463\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    464\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\amb_pop\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 819\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    820\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    821\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\amb_pop\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1048\u001B[0m             )\n\u001B[0;32m   1049\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1050\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1051\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1052\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\amb_pop\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1932\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1933\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0musecols\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1934\u001B[1;33m                 \u001B[0m_validate_usecols_names\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0musecols\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1935\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1936\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_parse_dates_presence\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\amb_pop\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_validate_usecols_names\u001B[1;34m(usecols, names)\u001B[0m\n\u001B[0;32m   1160\u001B[0m     \u001B[0mmissing\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mc\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0musecols\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mnames\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1161\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmissing\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1162\u001B[1;33m         raise ValueError(\n\u001B[0m\u001B[0;32m   1163\u001B[0m             \u001B[1;34mf\"Usecols do not match columns, columns expected but not found: {missing}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1164\u001B[0m         )\n",
      "\u001B[1;31mValueError\u001B[0m: Usecols do not match columns, columns expected but not found: [1, 2, 3, 4]"
     ]
    }
   ],
   "source": [
    "mae_df = pd.read_csv(\"../data/model1mae.csv\")\n",
    "mae_df = mae_df.rename(index={0:'1 Lag',1: '3 Lag',2: '7 Lag'})\n",
    "\n",
    "fig = mae_df.plot.bar(facet_col='variable')\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_layout(showlegend=False,\n",
    "                      title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "fig.write_image(\"../images/Model1mae.svg\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next few cells plot the validation line charts for each number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figs[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figs[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figs[2].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figs[3].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation seems to show that the model works fairly well with the shifts in lockdown variables if validating on the period from 1st January 2021.  It's computationally quite expensive to validate on too much data but it will show that the model can follow the general trend of changes.\n",
    "\n",
    "Changing the number of trees certainly helps it to run quicker, which is good to know seen as 200 seemed to be enough to produce a decent result.\n",
    "\n",
    "There are a few other reasons why it might take so much time.  The first is that it's a large dataset.  Reducing the amount of training data might help it run quicker.\n",
    "\n",
    "Furthermore, reducing the dimensionality of the data might also help.\n",
    "\n",
    "#### Variable Importance\n",
    "\n",
    "The following code repeats the data processing pipeline, fits the RandomForest model from above and extracts variable importance.\n",
    "\n",
    "The number of trees is set to 200 by default, this can be changed if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lags = [1,3,7]\n",
    "tree = 200\n",
    "importance_list = []\n",
    "for lag in lags:\n",
    "    n_in=lag\n",
    "    processed_data = (daily_footfall\n",
    "                      .pipe(start_pipeline)\n",
    "                      .pipe(series_to_supervised,n_in=n_in)\n",
    "                      .pipe(create_lockdown_predictors)\n",
    "                      .pipe(create_date_predictors)\n",
    "                          .pipe(create_holiday_predictors,bankhols,schoolterms)\n",
    "                          .pipe(create_weather_predictors, new_weather, previous_weather)\n",
    "                      .pipe(drop_na)\n",
    "                      .pipe(arrange_cols,n_in=n_in))\n",
    "\n",
    "    cols_to_scale = ['mean_temp', 'wind_speed', 'rain', 'hosp_indoor', 'hosp_outdoor', 'hotels', 'ent_indoor', 'ent_outdoor',\n",
    "                     'weddings', 'self_acc', 'sport_lei_indoor', 'sport_lei_outdoor', 'non_ess_retail', 'prim_sch',\n",
    "                     'sec_sch', 'uni_campus', 'outdoor_grp_public', 'outdoor_grp_private', 'indoor_grp', 'eat_out']\n",
    "\n",
    "    values = processed_data.values\n",
    "\n",
    "    data_cols_pos, data_cols = create_data_cols(processed_data)\n",
    "\n",
    "    #split into input and output columns\n",
    "    trainX, trainy = processed_data.iloc[:, :-1].copy(), processed_data.iloc[:, -1].copy()\n",
    "    trainX.loc[:,cols_to_scale] = min_max_scaler.fit_transform(trainX.loc[:,cols_to_scale])\n",
    "    #fit model\n",
    "    model = RandomForestRegressor(n_estimators=tree)\n",
    "    model.fit(trainX, trainy)\n",
    "    #extract feature importance\n",
    "    importance = create_importance_df(model.feature_importances_,data_cols,n_in)\n",
    "    importance_list.append(importance)\n",
    "\n",
    "importance_fig = []\n",
    "\n",
    "for df,lag in zip(importance_list,lags):\n",
    "    fig = df.head(10).plot.bar(color=df.head(10).index,\n",
    "                                           title=f\"Model 1 Variable Importance (Top 10) - {lag} time lagged\",\n",
    "                                           labels={'value': 'Variable Importance',\n",
    "                                                   'feature_name': 'Model Feature'})\n",
    "    fig.update_layout(showlegend=False,\n",
    "                      title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "    importance_fig.append(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot importance chart for 1 time lagged variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plot importance chart\n",
    "importance_fig[0].show()\n",
    "importance_fig[0].write_image(f\"../images/Model1Importance1.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot importance chart for 3 time lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plot importance chart\n",
    "importance_fig[1].show()\n",
    "importance_fig[1].write_image(f\"../images/Model1Importance3.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot importance chart for 7 time lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plot importance chart\n",
    "importance_fig[2].show()\n",
    "importance_fig[2].write_image(f\"../images/Model1Importance7.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance is a useful measure of assessing what predictors have strong power in predicting the outcome variable.  They do not necessarily explain why footfall changes at specific points but are a useful indicator of what is dominant within the model.  It may be that this can be used to reduce the dimensionality in the model for future iterations.\n",
    "\n",
    "Despite some reductions in the MAE of the model in validation when adding in more time lagged variables, they tend to dominate the model in terms of importance, squeezing out some explanatory lockdown inputs.\n",
    "\n",
    "Just using a single time lagged variable seems to be most appropriate for this model.\n",
    "\n",
    "Model #2 - Prediction of business as usual\n",
    "\n",
    "The following code repeats the initial data processing undertaken on the previous models, however omits the creation of lockdown variables.  This idea is to predict what footfall might have been like if the pandemic and lockdown measures didn't happen.  The initial pipeline steps through creating a supervised learning dataset and relevant footfall predictors.  Several tests have been run using different processes.\n",
    "\n",
    "This first block drops missing values from the data altogether, regardless of what columns these occur in.  In most checks, missing values only really occurred in weather related variables as data were not available.\n",
    "\n",
    "<b>Remember - Walk Forward Validation is potentially computationally expensive depending on how many tree/lag combinations you'd like to test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Run model 2 validation function using a range of trees\n",
    "lags = [1,3,7]\n",
    "trees = [100,200,500,1000]\n",
    "figs = []\n",
    "mae_df = pd.DataFrame()\n",
    "\n",
    "for tree in trees:\n",
    "    mae_list,validation_df,y = model2_validation(lags,daily_footfall,tree)\n",
    "    validation_df['y'] = y\n",
    "    fig = px.line(validation_df,title=f'Model 2A Validation - {tree} trees')\n",
    "    figs.append(fig)\n",
    "    fig.write_image(f\"../images/Model2AValidation{tree}.svg\")\n",
    "    mae_df[f'{tree}_trees'] = mae_list\n",
    "    mae_df.to_csv(f\"../data/model2Amae.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot the MAE for each combination of trees/lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mae_df = pd.read_csv(\"../data/model2Amae.csv\")\n",
    "mae_df = mae_df.rename(index={0:'1 Lag',1: '3 Lag',2: '7 Lag'})\n",
    "\n",
    "fig = mae_df.plot.bar(facet_col='variable')\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_layout(showlegend=False,\n",
    "                      title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "fig.write_image(\"../images/Model2mae.svg\")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show validation charts for each number of trees - These might all generate in one scrollable area.  Use the single shot code underneath the loop if you'd like each to be more visible."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for fig in figs:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Replace index to show specific figure\n",
    "figs[1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation above gives an indication of how accurate the model is on this run.  There may be differences each time it's run so the code could be developed to run a number of validations and see if the MAE changes much.  All forecasting is tricky, particularly with time series data, but the model seems to do a fairly good job at getting close to the predicted values for each day of the validation set.\n",
    "\n",
    "The next stage is to use the model above to make predictions on an 'out of sample' test set.  In this scenario, the data we want to predict on is everything after and including 16th March 2020, when large changes in footfall started to be seen after social distancing announcements.\n",
    "\n",
    "The same data processing pipeline is used, with train and test data split using the 16th March 2020 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "importance_list = []\n",
    "yhat_list = []\n",
    "tree = 200\n",
    "\n",
    "lags = [1,3,7]\n",
    "for lag in lags:\n",
    "    n_in=lag\n",
    "    # transform the time series data into supervised learning and add additional explanatory variables.  Filter to exclude period where lockdowns were in effect.\n",
    "    processed_data = (daily_footfall\n",
    "                      .pipe(start_pipeline)\n",
    "                      .pipe(series_to_supervised,n_in=n_in)\n",
    "                      .pipe(create_date_predictors)\n",
    "                          .pipe(create_holiday_predictors,bankhols,schoolterms)\n",
    "                          .pipe(create_weather_predictors, new_weather, previous_weather)\n",
    "                      .pipe(drop_na)\n",
    "                      .pipe(arrange_cols,n_in=n_in))\n",
    "\n",
    "    #Set columns to scale\n",
    "    cols_to_scale = ['mean_temp', 'wind_speed', 'rain']\n",
    "\n",
    "    #Create data col files to allow prediction outputs to be joined back to datetime later.\n",
    "    data_col_pos, data_cols = create_data_cols(processed_data)\n",
    "\n",
    "    #Split into training and testing datasets\n",
    "    train = processed_data.loc[processed_data.index < \"2020-03-16\"]\n",
    "    test = processed_data.iloc[processed_data.index >= \"2020-03-16\"]\n",
    "\n",
    "    #split training data into input and output columns\n",
    "    trainX, trainy = train.iloc[:, :-1].copy(), train.iloc[:, -1].copy()\n",
    "    #Fit and apply scaling to training data\n",
    "    trainX.loc[:,cols_to_scale] = min_max_scaler.fit_transform(trainX.loc[:,cols_to_scale])\n",
    "\n",
    "    #fit model\n",
    "    model = RandomForestRegressor(n_estimators=tree)\n",
    "    model.fit(trainX, trainy)\n",
    "\n",
    "    #split test data into input and output columns\n",
    "    testX, testy = test.iloc[:, :-1], test.iloc[:, -1]\n",
    "    #Apply scaler to test data\n",
    "    testX.loc[:,cols_to_scale] = min_max_scaler.transform(testX.loc[:,cols_to_scale])\n",
    "\n",
    "    #extract feature importance and join back to variable names\n",
    "    importance = create_importance_df(model.feature_importances_, data_cols,n_in)\n",
    "    importance_list.append(importance)\n",
    "\n",
    "\n",
    "\n",
    "    yhat_list.append(create_prediction_data(model.predict(testX),test))\n",
    "\n",
    "processed_data['roll_7_mean'] = processed_data['var1(t)'].rolling(7).mean()\n",
    "prediction_fig = daily_predicted_chart(yhat_list,processed_data)\n",
    "importance_fig = []\n",
    "\n",
    "for df,lag in zip(importance_list,lags):\n",
    "    fig = df.head(10).plot.bar(color=df.head(10).index,\n",
    "                                           title=f\"Model 2 Variable Importance (Top 10) - {lag} time lagged\",\n",
    "                                           labels={'value': 'Variable Importance',\n",
    "                                                   'feature_name': 'Model Feature'})\n",
    "    fig.update_layout(showlegend=False,\n",
    "                      title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "    importance_fig.append(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot predictions chart for 1,3 and 7 time lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = prediction_fig\n",
    "fig.show()\n",
    "fig.write_image(f\"../images/Model2PredictionA.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot importance chart for 1 time lagged variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plot importance chart\n",
    "importance_fig[0].show()\n",
    "importance_fig[0].write_image(f\"../images/Model2ImportanceA1.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot importance chart for 3 time lagged variables - Won't work if only 1 time lag has been included in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plot importance chart\n",
    "importance_fig[1].show()\n",
    "importance_fig[1].write_image(f\"../images/Model2ImportanceA3.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot importance chart for 7 time lagged variables - Won't work if only 1 time lag has been included in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plot importance chart\n",
    "importance_fig[2].show()\n",
    "importance_fig[2].write_image(f\"../images/Model2ImportanceA7.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot going on in those charts, but to summarise:\n",
    "- Introduction of more than one time lagged variable seems to throw the predictions off in a big way.\n",
    "- At 7 variables, the predictions mirror more what actually happened due to lockdown rather than what might have been.\n",
    "\n",
    "There's much less variation in the predictions than actual values from previous years.  There's almost an immediate drop after 16th March 2020 which would go against the trend seen in the data previously.  Maybe this is because seasonality and trends haven't been removed in pre-processing.  It's difficult to quantify any changes right now as I'm not particularly confident this works well.  There's also horrible artifacts in the weekly and monthly resamples caused by dropping entire weeks/months where weather data is missing.\n",
    "\n",
    "It might be worth looking at variable importance, removing some less powerful ones and potentially imputing some weather data.\n",
    "\n",
    "The following blocks of code rerun the analysis but with a process to impute weather data.  Note that as it stands the method isn't as robust as it could be.  Really the imputation should be done after splitting the train/test data for validation purposes, however I tried to do so and then rejoin the sets back together.  For some reason Python had problems with the test_train_split function within the validation so I abandoned this and just carried the imputing out as one dataset.  This means that there could be some data leakage but I'm happy to accept that for now as the scaling is still done after the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#transform the time series data into supervised learning and add additional explanatory variables.  Filter to exclude period where lockdowns were in effect.\n",
    "processed_data = (daily_footfall\n",
    "                  .pipe(start_pipeline)\n",
    "                  .pipe(series_to_supervised,n_in=6)\n",
    "                  .pipe(create_date_predictors)\n",
    "                          .pipe(create_holiday_predictors,bankhols,schoolterms)\n",
    "                          .pipe(create_weather_predictors, new_weather, previous_weather)\n",
    "                  .pipe(arrange_cols))\n",
    "\n",
    "#lists of columns to scale and impute (may or may not be different)\n",
    "cols_to_scale = ['mean_temp', 'wind_speed', 'rain']\n",
    "cols_to_impute = ['mean_temp', 'wind_speed', 'rain']\n",
    "\n",
    "#set processed data as pre lockdown period\n",
    "processed_data = processed_data.loc[processed_data.index < \"2020-03-16\"]\n",
    "#interpolate columns with missing values using a time based method.\n",
    "processed_data = processed_data.interpolate(method='time')\n",
    "#drop missing values\n",
    "processed_data = processed_data.dropna()\n",
    "\n",
    "#DEPRECATED CODE - initially split, impute and then rejoin but it was causing an indexing error within the walk forward validation method.\n",
    "# test_rows = 30\n",
    "# train, test_rows = train_test_split(processed_data,test_rows)\n",
    "#\n",
    "# train.loc[:,cols_to_impute], test_rows.loc[:,cols_to_impute] = train.loc[:,cols_to_impute].interpolate(method='time'), test_rows.loc[:,cols_to_impute].interpolate(method='time')\n",
    "# train,test_rows = train.dropna(),test_rows.dropna()\n",
    "#\n",
    "# prelockdown_data = pd.concat([train,test_rows],axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rest of the modelling including the weather imputed variables needs to be added back into the notebook.  I initially ran all the models only using a single value for trees and time lags.  Since then I've added in the multiple tests.  Just need to adapt the new code for this section.\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Due to the way in which the validation has been set up, it's difficult to fit it into the 'conventional' scikitlearn pipeline and evaluate the hyperparameters so I'll leave those for now.  Tweaking them might result in different predictions but it might just be that this model isn't the best/will take a lot of effort to get something usable.\n",
    "\n",
    "I've spent a bit of time tidying up the code above to run the preprocessing through pipes, but not necessarily the SciKit learn pipelines used explicitly for modelling.  Due to the transformation of several variables into standardised/normalised ranges prior to splitting the data, I'll need to come back to this and adjust it so that only the variables are created at this point and any adjustments (such as scaling or imputing) are undertaken at the correct time.\n",
    "\n",
    "The scaling is potentially still an issue as it's only applied once throughout the validation process.  There is a solution, listed below, but it might not be worth the time trying to implement it if a seperate ARIMA/Prophet workflow is developed:\n",
    "    - After seeding the history list, pull the code to convert into an array and split off the X and y variables.\n",
    "    - Within the loop, split off test X and y.  Don't isolate the single row in the loop, instead do it inside the random forest function.\n",
    "    - Instead of feeding history into the random forest function, feed in train X and y as they are.\n",
    "    - Fit the model inside the random forest function and apply scaling to test X.  isolate the single row the loop is currently on and then predict on that.\n",
    "    - After appending the last observation to the history, turn it into an array, split off train X and scale it once again.\n",
    "    - Return to the start of the loop with a new, scaled train X and repeat, refitting the model on this and rescaling test X after.\n",
    "\n",
    "The overarching problem with this method of modelling using a RF is the amount of custom code and script required to develop more feasible and robust workflows.  As illustrated above, making sure that data leakage is avoided during scaling isn't considered in the intial validation code and could be an enormous time sink going forward.\n",
    "\n",
    "Future work to be developed on RF if desired:\n",
    "- Adjust scaling algorithm to retrain everytime the validation makes a one step prediction.\n",
    "- Undertake dimensionality reduction using an appropriate method\n",
    "- Remove outliers caused by 'one-off' events or odd days.  Include some of these events as explanatory variables.\n",
    "- Do a location based analysis/modelling to take into account different areas of the city\n",
    "- More research into the feasibility of Random Forest for Time Series Modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}